# Training configuration
gamma: 0.99  # Discount factor
lambda_gae: 0.95  # GAE lambda
clip_epsilon: 0.2  # PPO clipping parameter
value_loss_coef: 0.5
entropy_coef: 0.00025
max_grad_norm: 0.5
target_kl: 0.015
use_action_mask: true  # Whether to use action masking
# Optimizer configuration
optim:
  opt_name: lamb
  max_lr: 5e-4
  betas:
    - 0.9
    - 0.999
  eps: 1e-6
  weight_decay: 0.01
  warmup_steps_ratio: 0.025
  scheduler_names: 
    - constant
    - linear
  blacklist_weight_modules:
    - norm
    - embedding

# Resume training configuration
resume_from_checkpoint: null  # Path to checkpoint to resume from (null = start fresh)
resume_extend_steps: true  # If true, extend training by total_timesteps; if false, train until total_timesteps

# Training loop parameters
total_timesteps: 1000000000
rollout_batch_size: 512  # Parallel environments
rollout_batches: 10  # Number of rollout batches per iteration
update_epochs: 4  # Epochs per policy update
train_batch_size: 2048
save_freq: 100000000
max_steps: 500000

# Buffer configuration
buffer_size: 100000
